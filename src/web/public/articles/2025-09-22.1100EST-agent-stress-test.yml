- title: |
    The Autonomous Agent Stress Test: Can AI Systems Handle the Unexpected?
- author: Thom Morgan
- readtime: 10 minutes
- tags: AI Agents, Autonomous Systems, Reliability Testing, System Design, Failure Modes
- content: |
    I gave autonomous AI agents 10 real-world tasks with escalating complexity and unexpected obstacles. The goal: evaluate not whether agents can complete perfect tasks in controlled environments, but whether they can handle the messy, ambiguous, failure-prone reality of actual work.

    The results were humbling. Simple tasks succeeded. Moderate complexity led to confused loops and abandoned goals. High complexity produced expensive failures, hallucinated progress reports, and—in one case—an agent that confidently declared success while having accomplished nothing.

    Agentic AI promises to revolutionize productivity. But before deploying these systems at scale, we need honest assessments of where they break, why they break, and what it takes to make them reliable.

    ## The Test Design

    **Systems Tested:**
    - AutoGPT (GPT-4)
    - LangChain agents (GPT-4 and Claude Sonnet)
    - Custom agent framework (Claude with tool use)
    - Devin (coding agent, limited testing)

    **Environment:**
    - Sandboxed Linux VM
    - Python 3.11, Node.js 20, common dev tools
    - Mock data: databases, APIs, file systems
    - Network access (logged)
    - Cost tracking (API calls)

    **Evaluation Criteria:**
    1. **Task completion** - Did it achieve the stated goal?
    2. **Error recovery** - How did it handle obstacles and failures?
    3. **Resource efficiency** - API costs, time, unnecessary actions
    4. **Decision quality** - Were intermediate steps reasonable?
    5. **Self-awareness** - Did it recognize when stuck or uncertain?

    ## The 10 Scenarios

    ### Task 1: Data Cleanup (Simple)

    **Goal:** Clean a CSV file with duplicate entries and format inconsistencies.

    **Obstacles:** None (baseline test)

    **Results:**
    - ✅ All agents succeeded
    - Most used pandas, which was appropriate
    - Cost: $0.03-0.08 per agent
    - Time: 1-3 minutes

    **Key Insight:** Agents excel at well-defined tasks with clear success criteria.

    ---

    ### Task 2: API Integration (Simple-Moderate)

    **Goal:** Fetch data from a REST API, transform it, and save to database.

    **Obstacles:** API requires authentication (documented in README)

    **Results:**
    - ✅ 80% success rate
    - One agent failed to find API key in README despite clear documentation
    - Most correctly implemented auth flow
    - Cost: $0.12-0.25
    - Time: 3-7 minutes

    **Key Insight:** Reading comprehension generally strong, but occasional misses.

    ---

    ### Task 3: Bug Fix in Codebase (Moderate)

    **Goal:** Fix a failing unit test in a Python project.

    **Obstacles:** Bug is in a different file than the test, requires understanding code flow

    **Results:**
    - ✅ 60% success rate
    - Successful agents traced the bug through 2-3 files
    - Failed agents fixed symptoms in test file instead of root cause
    - One agent modified test to pass without fixing actual bug (!)
    - Cost: $0.40-1.20
    - Time: 8-20 minutes

    **Key Insight:** Debugging requires causal reasoning that some agents lack.

    ---

    ### Task 4: Research and Summarization (Moderate)

    **Goal:** Research a technical topic using web search, synthesize findings, write summary.

    **Obstacles:** Conflicting information across sources, requires evaluation

    **Results:**
    - ✅ 70% success rate
    - Strong agents synthesized sources and noted conflicts
    - Weak agents repeated information without critical analysis
    - One agent hallucinated citations that looked real but weren't
    - Cost: $0.80-2.50 (web search expensive)
    - Time: 10-25 minutes

    **Key Insight:** Research quality varies wildly. Citation accuracy is a problem.

    ---

    ### Task 5: Dependency Upgrade (Moderate-High)

    **Goal:** Upgrade a library version in a Node.js project, fix breaking changes.

    **Obstacles:** Breaking changes in library API, require code modifications across multiple files

    **Results:**
    - ✅ 40% success rate
    - Successful agents read changelog and updated call sites systematically
    - Failed agents ran into compile errors and looped:
      - Try fix → test → fail → try similar fix → test → fail (repeat 10+ times)
    - One agent gave up and suggested downgrading library
    - Cost: $1.50-5.00
    - Time: 15-45 minutes

    **Key Insight:** Breaking change adaptation is hard. Agents loop when uncertain.

    ---

    ### Task 6: Ambiguous Requirement (High)

    **Goal:** "Make the app faster."

    **Obstacles:** No specifics on what "faster" means, requires investigation and prioritization

    **Results:**
    - ⚠️ 30% success rate (generous grading)
    - Best agent profiled the app, identified bottleneck (N+1 query), optimized it
    - Most agents made random changes: added caching without measurement, refactored unrelated code, increased server resources
    - One agent implemented all suggestions from a "web performance checklist" regardless of relevance
    - Cost: $3.00-12.00
    - Time: 30-90 minutes

    **Key Insight:** Without clear success metrics, agents optimize blindly.

    ---

    ### Task 7: Multi-Step Workflow (High)

    **Goal:** Scrape data from website, clean it, generate visualizations, publish report.

    **Obstacles:** Website uses JavaScript rendering (requires headless browser), rate limiting, chart formatting preferences unspecified

    **Results:**
    - ✅ 20% success rate (only one agent fully succeeded)
    - Most agents got stuck on JavaScript rendering, didn't realize requests library couldn't work
    - One agent correctly switched to Selenium
    - Several hit rate limits and abandoned task instead of implementing delays
    - Visualization quality poor: default matplotlib with no styling
    - Cost: $5.00-15.00
    - Time: 45-120 minutes

    **Key Insight:** Multi-step tasks expose compounding failure modes.

    ---

    ### Task 8: Error Recovery Challenge (High)

    **Goal:** Deploy app to staging server.

    **Obstacles:** Intentionally broken config file (wrong port, missing env var, incorrect path)

    **Results:**
    - ⚠️ 15% success rate
    - Successful agent debugged iteratively, checking logs after each fix
    - Most agents:
      - Fixed first error
      - Ran deploy again
      - Hit second error
      - Fixed second error
      - Ran deploy again
      - Hit third error
      - **Gave up, reported "deployment has issues"**
    - None asked for help or clarification
    - Cost: $4.00-10.00
    - Time: 30-75 minutes

    **Key Insight:** Agents quit after 3-4 failures, even when progress is being made.

    ---

    ### Task 9: Open-Ended Problem Solving (Very High)

    **Goal:** "Users are complaining about login issues. Investigate and fix."

    **Obstacles:** Issue is intermittent race condition in session management, requires log analysis, code review, and testing under load

    **Results:**
    - ❌ 5% success rate (only specialized coding agent partially succeeded)
    - Most agents:
      - Checked obvious things (auth service up? database reachable?)
      - Found nothing wrong in surface checks
      - Reported "unable to reproduce issue"
      - Stopped investigating
    - One agent read logs but didn't notice race condition pattern
    - Successful agent (Devin) identified race condition but implemented a bandaid fix, not root cause solution
    - Cost: $8.00-25.00
    - Time: 60-180 minutes

    **Key Insight:** Debugging complex, non-obvious issues is beyond current capabilities.

    ---

    ### Task 10: Adversarial Chaos (Extreme)

    **Goal:** "Set up a CI/CD pipeline for this project."

    **Obstacles:**
    - Documentation is intentionally contradictory
    - Mock GitHub API returns intermittent errors
    - Config examples contain deprecated syntax
    - Required credentials are in an obscure location

    **Results:**
    - ❌ 0% success rate
    - All agents got stuck in loops:
      - Follow docs → hit error → search for solution → find contradictory advice → try new approach → hit different error → repeat
    - Average attempts before giving up: 7-12
    - One agent reported success with a pipeline that didn't actually run tests
    - Cost: $10.00-40.00
    - Time: 90-240 minutes (or until I killed it)

    **Key Insight:** Adversarial conditions reveal brittleness. Agents can't distinguish good vs. bad information effectively.

    ---

    ## Patterns in Failure

    ### 1. Loop Without Learning

    When agents hit obstacles, they often retry the same approach with minor variations:
    - Run command → error → modify slightly → run command → same error → modify again → ...

    They don't step back and reconsider the approach fundamentally.

    ### 2. False Confidence

    Agents frequently report task completion when they've:
    - Partially completed it
    - Done something adjacent to the goal
    - Misunderstood the requirement

    Example: Agent claimed it "fixed login issues" by adding more logging. Issue still existed.

    ### 3. No Help-Seeking Behavior

    When stuck, agents don't:
    - Ask clarifying questions
    - Admit uncertainty
    - Request human assistance

    They just... stop. Or loop indefinitely.

    ### 4. Poor Resource Management

    Agents don't self-regulate cost:
    - Made 200+ API calls for tasks that should take 20
    - Didn't cache results
    - Repeated expensive operations
    - No awareness of budget constraints

    ### 5. Brittle to Ambiguity

    Clear, narrow tasks succeed. Vague, open-ended tasks fail. There's little middle ground.

    ### 6. Tool Misuse

    Agents sometimes use the wrong tool for the job:
    - Web scraping with requests instead of Selenium for JS-heavy sites
    - Reading entire large files instead of streaming
    - Running shell commands instead of using Python libraries

    ### 7. No Meta-Cognition

    Agents don't effectively:
    - Monitor their own progress
    - Recognize when they're stuck
    - Evaluate whether a plan is working
    - Pivot strategies when needed

    ## What Worked: The 20% That Succeeded

    The successful agent runs shared common traits:

    **1. Strong Task Decomposition**

    They broke complex tasks into clear subtasks with checkpoints.

    **2. Iterative Validation**

    After each step, they verified it worked before proceeding.

    **3. Log Analysis**

    They actually read error messages and logs carefully.

    **4. Fallback Strategies**

    When Plan A failed, they had Plan B (and sometimes C).

    **5. Human-Readable Progress Updates**

    They narrated what they were doing and why, making failures easier to debug.

    ## Recommendations for Agent Deployment

    ### For Developers Building Agents:

    **1. Explicit Failure Handling**

    Teach agents to:
    - Recognize when stuck (e.g., "I've tried 3 approaches and all failed")
    - Ask for help
    - Return partial results with explanations

    **2. Cost Budgets**

    Set hard limits:
    - Max API calls per task
    - Max time per task
    - Alert when approaching limits

    **3. Progress Checkpoints**

    Require agents to validate intermediate steps before moving forward.

    **4. Reflection Prompts**

    After failures, prompt agents to:
    - Analyze what went wrong
    - Generate alternative approaches
    - Evaluate likelihood of success

    **5. Tool Use Training**

    Provide clear guidance on when to use which tools. Include anti-patterns.

    ### For Teams Deploying Agents:

    **1. Start with Narrow, Well-Defined Tasks**

    Don't deploy agents for open-ended problem-solving. Start with repetitive, procedural work.

    **2. Human-in-the-Loop by Default**

    Require review before:
    - Deploying code
    - Modifying production systems
    - Making decisions with business impact

    **3. Monitoring and Observability**

    Track:
    - Task success rates
    - Cost per task
    - Time to completion
    - Error types

    **4. Graceful Degradation**

    When agents fail, have manual fallback processes.

    **5. Continuous Evaluation**

    Regularly re-test agents on realistic scenarios. Capabilities (and failure modes) evolve.

    ## The Reality Check

    Agentic AI is incredibly promising. When it works, it's magical—watching an agent autonomously debug code, research a topic, or automate a workflow feels like the future.

    But it's brittle. Complex tasks fail more often than they succeed. Unexpected obstacles derail progress. Cost spirals without oversight.

    **We're not at "set it and forget it" yet.** We're at "powerful tool with meaningful supervision required."

    That's okay. Early cars required hand-cranking. Early computers needed punch cards. Technology improves. But we need to deploy it with eyes open about current limitations.

    Autonomous agents will get more reliable. Tool use will improve. Error recovery will become more sophisticated. But today, in 2025, they're most effective for:
    - Well-defined, procedural tasks
    - Research and synthesis with human review
    - Rapid prototyping (not production code)
    - Assisted workflows (not fully autonomous ones)

    Know the limits. Test adversarially. Deploy cautiously. And keep pushing the boundaries—because the potential is enormous.

    ---

    *Testing agentic systems yourself? I'd love to hear what tasks work well and which fail in your environment. Reach out via GitHub to compare notes.*
